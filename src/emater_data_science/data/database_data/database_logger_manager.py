import threading
import polars as pl
from emater_data_science.data.database_data.central_database_connection import CentralDatabaseConnection
from emater_data_science.logging.logging_table_model import LoggingTable  # SQLAlchemy table for logs

class DatabaseLoggerManager:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(DatabaseLoggerManager, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if hasattr(self, "_initialized") and self._initialized:
            return
        self.buffer_size = 30
        self.flush_interval = 1.0
        self.buffer = []
        self.lock = threading.Lock()
        self.stop_event = threading.Event()
        self.flush_thread = None
        self._is_shutting_down = False  # Flag to prevent further logs once shutdown starts.
        self._initialized = True
        self._initialize()

    def _initialize(self) -> None:
        self.stop_event.clear()  # Ensure the stop event is cleared.
        if self.flush_thread is None or not self.flush_thread.is_alive():
            self.flush_thread = threading.Thread(target=self._flush_periodically, daemon=True)
            self.flush_thread.start()

    def fStoreLog(self, log) -> None:
        """
        Store a log entry if shutdown has not been initiated.
        Logs generated by operations already enqueued will be stored.
        Any attempt to store new logs after shutdown will print a warning and discard the log.
        """
        if self._is_shutting_down:
            print(
                "DATABASE LOGGER MANAGER WARNING: Attempted to store log after shutdown; log discarded.",
                log.level, log.message, log.variablesJson
            )
            return
        with self.lock:
            self.buffer.append(log)
            if len(self.buffer) >= self.buffer_size:
                self._flush()

    def _flush(self) -> None:
        """
        Flush the current buffer of logs by converting them into a Polars DataFrame and
        writing them via the CentralDatabaseConnection.
        """
        with self.lock:
            if not self.buffer:
                return
            logs_to_write = self.buffer.copy()
            self.buffer.clear()
        logs_dicts = [
            {
                "level": log.level,
                "message": log.message,
                "variablesJson": log.variablesJson
            }
            for log in logs_to_write
        ]
        df = pl.DataFrame(logs_dicts)
        CentralDatabaseConnection().fWrite(model=LoggingTable, data=df)

    def _flush_periodically(self) -> None:
        """
        Periodically flush the log buffer until a shutdown is signaled.
        After shutdown, perform one final flush.
        """
        while not self.stop_event.is_set():
            if self.stop_event.wait(timeout=self.flush_interval):
                break
            self._flush()
        # Final flush after the stop event is set.
        self._flush()

    def fShutdown(self) -> None:
        """
        Initiate shutdown:
         - Set the shutdown flag so that no new logs are accepted.
         - Signal the flush thread to stop and wait for it to finish.
         - The final flush ensures that all logs enqueued before shutdown are written.
        """
        self._is_shutting_down = True
        self.stop_event.set()
        if self.flush_thread:
            self.flush_thread.join()
            self.flush_thread = None
